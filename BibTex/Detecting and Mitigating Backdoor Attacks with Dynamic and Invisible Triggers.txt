@InProceedings{10.1007/978-3-031-30111-7_19,
author="Zheng, Zhibin
and Hua, Zhongyun
and Zhang, Leo Yu",
editor="Tanveer, Mohammad
and Agarwal, Sonali
and Ozawa, Seiichi
and Ekbal, Asif
and Jatowt, Adam",
title="Detecting and Mitigating Backdoor Attacks with Dynamic and Invisible Triggers",
booktitle="Neural Information Processing",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="216--227",
abstract="When a deep learning-based model is attacked by backdoor attacks, it behaves normally for clean inputs, whereas outputs unexpected results for inputs with specific triggers. This causes serious threats to deep learning-based applications. Many backdoor detection methods have been proposed to address these threats. However, these defenses can only work on the backdoored models attacked by static trigger(s). Recently, some backdoor attacks with dynamic and invisible triggers have been developed, and existing detection methods cannot defend against these attacks. To address this new threat, in this paper, we propose a new defense mechanism that can detect and mitigate backdoor attacks with dynamic and invisible triggers. We reverse engineer generators that transform clean images into backdoor images for each label. The generated images by the generator can help to detect the existence of a backdoor and further remove it. To the best of our knowledge, our work is the first work to defend against backdoor attacks with dynamic and invisible triggers. Experiments on multiple datasets show that the proposed method can effectively detect and mitigate the backdoor with dynamic and invisible triggers in deep learning-based models.",
isbn="978-3-031-30111-7"
}

